\chapter{Metodología}
\label{cap:metodologia}

Las principales herramientas utilizadas en este trabajo han sido las siguientes.

\begin{itemize}
  \item \LaTeX \, como procesador de textos científico.
  \item \emph{Scala}, \emph{Spark} y \emph{MLlib} como ecosistema para el
  tratamiento de datos en entornos \emph{Big Data}.
  \item \emph{R} y \emph{ggplot2} para graficar resultados. Junto a \emph{knitr},
  este documento sigue el paradigma de \emph{programación literaria},
  uniendo documento y programa, siendo muy cómodo de escribir y leer.
  \item MariaDB como sistema gestor de bases de datos relacionales (SQL).
  \item Git como sistema de control de versiones, tanto para el software como
  para la memoria. Todo está disponible en \fnurl{GitHub}{https://github.com/analca3/StarCraft-winner-prediction}.
\end{itemize}

Se han escogido estas herramientas por ser el estado del arte en cada uno de los
ámbitos para los que fueron desarrolladas. En particular, con \emph{Scala}
y \emph{Spark} se consigue tratamiento de datos en entornos \emph{Big Data} de manera
transparente al usuario, ofreciendo un desarrollo similar en entornos
centralizados y distribuidos.

\section{Selección de los datos}
\label{sec:seleccion}

Los datos utilizados se han cogido de \citep{DBLP:conf/flairs/RobertsonW14},
que con su trabajo ofrecen seis bases de datos relacionales de partidas dos
contra dos, con las distintas combinaciones de razas que ofrece el juego.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figure/Robertson14DatabaseDiagram.pdf}
  \caption{Diagrama Entidad-Relación que modela cada una de las bases de datos.}
  \label{fig:database}
\end{figure}

En la figura~\ref{fig:database} se puede ver el diagrama Entidad-Relación de
las bases de datos que contienen las partidas. En primer lugar hay que
comprender los datos que se tienen para saber de qué conocimiento se parte.
Este paso ha sido sencillo gracias a lo dicho en el capítulo~\ref{cap:objetivos},
el que el trabajo de \citep{DBLP:conf/flairs/RobertsonW14} sea libre facilita
mucho esta tarea. La mayoría de características comparten nombre con atributos
propios del juego extraídos de la API más utilizada para trabajar con
\emph{StarCraft}, la \fnurl{\emph{BWAPI}}{http://bwapi.github.io/}. Otros datos
son derivados que han calculado los investigadores, como la distancia a la base
en un determinado momento.

En siguiente lugar, hay que ver
qué variables se usan y cómo se organizan. El objetivo de este
paso es conseguir una tabla de la forma filas por columnas, de manera que pueda
ser interpretable por cualquier algoritmo clásico de aprendizaje.

La propuesta es la siguiente: cada fila del conjunto de datos va a ser un
instante de la partida, que estará compuesto por los recursos de cada jugador.
Es un enfoque distinto a la mayoría de los que se trabajan, como se ha comentado
en el capítulo~\ref{cap:introduccion}.
A primera vista parece sencillo, pero se verá que conseguir información sobre
las unidades de cada jugador no va a ser tarea fácil.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figure/Robertson14DatabaseDiagramSeleccion.pdf}
  \caption{Variables seleccionadas de las bases de datos.}
  \label{fig:databaseSelection}
\end{figure}

En la figura~\ref{fig:databaseSelection} se pueden ver las variables
seleccionadas, que se explican en las siguientes líneas. Posteriormente se
explicará cómo se han extraído dichas características.

Las características seleccionadas son, principalmente, los recursos de cada
jugador, sus unidades (que se cuantifican no según el número sino según la
suma de unos determinados valores que tiene cada unidad, dependiendo de su
importancia) y sus edificios, además de las observaciones de estos valores que
tiene un jugador del otro. Por último, también se tienen en cuenta el valor
que cada jugador cree que queda de los recursos del mapa.

Los detalles de las características seleccionadas según cada tabla son:

\begin{itemize}
  \item replay: Esta tabla contiene datos asociados a cada partida.
  \begin{itemize}
    \item ReplayID: Identificador de cada partida.
    \item Duration: Duración (en frames) de cada partida. 15 frames equivalen a 1 segundo.
  \end{itemize}
  \item playerreplay: Esta tabla contiene datos asociados a un jugador en una partida.
  \begin{itemize}
    \item PlayerReplayID: Identificador de un jugador en una partida.
    \item ReplayID: Identificador de partida asociado.
    \item Winner: Ganador de cada partida.
  \end{itemize}
  \item resourcechange: Esta tabla contiene datos asociados a cambios en los recursos de un jugador.
  \begin{itemize}
    \item PlayerReplayID: Identificador del jugador que produce un cambio.
    \item Frame: Frame en el que se produce un cambio.
    \item Minerals: Cantidad de minerales que tiene un jugador en ese momento.
    \item Gas: Cantidad de gas que tiene un jugador en ese momento.
    \item Supply: Capacidad de carga del jugador.
    \item TotalMinerals: Cantidad total de minerales que ha obtenido un jugador, sin contar gastos.
    \item TotalGas: Cantidad total de gas que ha obtenido un jugador, sin contar gastos.
    \item TotalSupply: Capacidad que ha obtenido un jugador, sin contar gastos.
  \end{itemize}
  \item regionvaluechange: Esta tabla contiene datos asociados a cambios de un jugador
  en una región del mapa determinada. Cada \emph{value}, que llamaremos de aquí en adelante
  \emph{valor}, es la suma del precio de una unidad en Minerales y Gas.
  \begin{itemize}
    \item PlayerReplayID: Identificador del jugador que produce un cambio.
    \item RegionID: Identificador de la región del mapa donde se produce un cambio.
    \item Frame: Frame en el que se produce el cambio.
    \item GroundUnitValue: Valor de las unidades terrestres en esta región.
    \item BuildingValue: Valor de las construcciones en esta región.
    \item AirUnitValue: Valor de las unidades aéreas en esta región.
    \item EnemyGroundUnitValue: Valor de las unidades terrestres del enemigo en esta región.
    Este valor es estimado, sólo se conoce lo que el jugador puede ver del enemigo.
    \item EnemyBuildingValue: Valor de las construcciones del enemigo en esta región.
    Este valor es estimado, sólo se conoce lo que el jugador puede ver del enemigo.
    \item EnemyAirUnitValue: Valor de las unidades aéreas del enemigo en esta región.
    Este valor es estimado, sólo se conoce lo que el jugador puede ver del enemigo.
    \item ResourceValue: Valor de los recursos en esta región. Este valor es estimado,
    sólo se conoce lo que el jugador puede ver del mapa. Si el jugador no conoce una zona, estima que los recursos restantes es la totalidad de lo disponible en la región.
  \end{itemize}
\end{itemize}

Una vez decididas las variables que vamos a tomar, se pasará a manejar las
bases de datos para conseguirlas de la manera más limpia posible. Las variables
escogidas quedan resumidas en tabla~\ref{tab:features}.

<<features>>=
if(! exists("data.full")) data.full <- read.csv("../../data/data_full.csv", colClasses=c("factor","integer",rep("numeric",27),"factor","factor"))
knitr::kable(colnames(data.full), format = "latex", caption = "Tabla con características seleccionadas y con su orden")
@

\section{Preprocesamiento y transformación de los datos}
\label{sec:preprocesamiento}

Como se ha comentado anteriormente, los datos vienen organizados en seis bases
de datos relacionales, de tipo SQL, por lo que se hace necesario el uso de un
\emph{Sistema Gestor de Bases de Datos}. Como se comentó al principio de esta
sección, se ha elegido \emph{MariaDB} por ser la implementación libre de la
archiconocida \emph{MySQL}. No se detallará
el montaje y puesta en marcha de esto, aunque es de gran importancia
hacer saber que para cualquier científico de datos, manejarse con el
levantamiento de este tipo de servicios también es muy importante.

El procedimiento para extraer las partidas no es más que una consulta para cada
partida de la base de datos. Esta consulta es algo compleja, por lo que se
intentará explicar de la manera más clara posible.

Lo primero será concentrarse en una partida: después se iterará sobre todas
con un procedimiento. Se podría haber resuelto todo con una sola consulta, pero
habría complicado un poco más su escritura y depuración. Una vez tenemos un
\emph{ReplayID} en concreto, se toma su duración \emph{Duration} y los
jugadores que compiten en ella \emph{PlayerReplayID}. Se tomará como el primero
el que tenga un \emph{PlayerReplayID} más pequeño, aunque se verá más adelante
que no habrá ninguna ventaja para ningún jugador.

Ahora hay que tomar una decisión importante. Tras un primer vistazo a los datos,
se observa que los instantes de tiempo \emph{Frame} en los que se registra
algún cambio no tienen por qué ser ni siquiera
parecidos, ni entre jugadores ni entre cambios de unidades o recursos.
Por tanto, se procede de la siguiente manera: para no perjudicar a ninguno de
los jugadores, en el conjunto de datos final los instantes de tiempo tomados
como referencia serán aquellos instantes en los que exista un cambio en los
recursos, que no unidades, de cada uno de los dos jugadores.

Esto implica directamente otra pequeña decisión al diseñar la consulta. Como
se ha comentado, los instantes de tiempo de los cambios en las unidades de
un jugador no tienen por qué coincidir con los cambios en recursos (es más,
es muy poco probable que coincidan exactamente). Por lo tanto, al tomar como
referencia los instantes de tiempo \emph{Frame} de cambios de recursos, hay
que buscar la manera de tomar el estado actual de las unidades de cada jugador
más precisa posible en el instante determinado. Esto se soluciona fácilmente
tomando el último valor de las unidades en el tiempo: se tomará el valor de
las unidades en el instante de tiempo más grande posible, pero más pequeño
que el actual.

Una vez se tienen las referencias en el tiempo, falta un detalle importante:
como se ha comentado en la sección~\ref{sec:seleccion}, los cambios registrados
en los valores de las unidades son a nivel de una determinada región del mapa.
Por tanto, para tomar el valor total de las unidades de un jugador, hay que
sumar los valores en todas las regiones del mapa.

Además de todo esto, existe una última cuestión que se ha tenido que resolver.
Al tomar datos de un jugador en un instante de tiempo para el que dicho jugador
no tiene información, dichos datos desconocidos se rellenan con \texttt{NULL}.
Sería sencillo dejarlos así y, en una siguiente fase, imputar dichos datos
perdidos con cualquier algoritmo clásico. Pero existe una solución mejor y
mucho más lógica para este problema: si para un jugador un dato no existe
porque no ha habido un cambio en ese instante de tiempo, significa que tendrá
los mismos recursos/unidades/etc que en su anterior cambio. Por tanto, sólo
hay que tomar el valor no nulo anterior más próximo para cada una de las
variables de la tabla.

Una vez con todos los datos bien formados, sólo hay que añadir el
\emph{ReplayID}, la duración de la partida \emph{Duration}, el ganador
\emph{Winner} y las razas implicadas. De esta manera ya tenemos todos los
datos de una partida bien formados, sin valores perdidos y listos para ser
tratados como un conjunto de datos sobre cualquier software. Como se ha
comentado anteriormente, sólo hay que iterar por los distintos \emph{ReplayID}
para conseguir los datos de todas las partidas de cada una de las seis bases
de datos.

Con todo esto, se puede comenzar con un análisis
exploratorio de datos.

<<loadReplaysFacet>>=
if(! file.exists("../../data/replays.melt.plots.rds")) {
  if(! exists("data.full")) data.full <- read.csv("../../data/data_full.csv", colClasses=c("factor","integer",rep("numeric",27),"factor","factor"))

  replays.plot <- data.full[data.full$ReplayID %in% sample(unique(data.full$ReplayID[grepl("PvP",data.full$ReplayID)]),3),]

  features.melt <- list(c("Minerals","Gas","Supply"),
                        c("TotalMinerals","TotalGas","TotalSupply"),
                        c("GroundUnitValue","BuildingValue","AirUnitValue"),
                        c("ObservedEnemyGroundUnitValue","ObservedEnemyBuildingValue","ObservedEnemyAirUnitValue"),
                        c("ObservedResourceValue"))

  replays.melt.plots <- lapply(features.melt, function(features){
    r1 <- melt(replays.plot, id.vars = c("Frame","ReplayID"), measure.vars = features, variable_name = "Value")
    r2 <- melt(replays.plot, id.vars = c("Frame","ReplayID"), measure.vars = paste0("Enemy",features), variable_name = "Value")
    replays.melt <- merge(r1,r2, by = c("Frame","ReplayID"))
    replays.melt <- replays.melt[replays.melt$variable.y == paste0("Enemy",replays.melt$variable.x),
                                 !colnames(replays.melt) %in% "variable.y"]
    colnames(replays.melt) <- c("Frame","ReplayID","Variable","Value","Value2")

    gg <- ggplot(replays.melt) + facet_grid(ReplayID ~ Variable) + geom_line(aes(x = Frame, y = Value),  color = "red") + geom_line(aes(x = Frame, y = Value2), color = "blue")
    gg
  })

  saveRDS(replays.melt.plots, "../../data/replays.melt.plots.rds")
}
replays.melt.plots <- readRDS("../../data/replays.melt.plots.rds")
@

<<loadMetadata>>=
if (! file.exists("../../data/metadata.csv")) {
  metadata <- data.full[, colnames(data.full) %in% c("ReplayID", "Duration", "Races")]
  metadata <- unique(metadata)
  write.csv(metadata, "../../data/metadata.csv", row.names = FALSE)
}
metadata <- read.csv("../../data/metadata.csv")
@

En primer lugar se incluye en la figura~\ref{fig:replaysHistogram} un
gráfico de barras con la duración de las partidas
del dataset. Incluye la distribución de las 6 posibles combinaciones de
razas que se pueden dar en la partida.

<<replaysHistogram, fig.cap = "Duración de las partidas del dataset">>=
metadata <- read.csv("../../data/metadata.csv")
gg <- ggplot(data=metadata) +
  geom_bar(aes(x=ReplayID,y=Duration,fill=Races), stat="identity", width = 0.75) +
  geom_hline(yintercept = mean(metadata$Duration), color = "red",linetype="dashed") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
gg
@

Se puede observar que la duración de las partidas parece depender de la
combinación de razas enfrentadas en ésta. La duración no es un atributo predictor
que se vaya a usar, pero es posible que las razas implicadas tengan peso
en los modelos finales.

\newpage

Ahora se tomarán tres partidas de manera aleatoria, y se echará un vistazo
preliminar a la forma en la que se desarrollan. Hay que tener en cuenta que son
tres partidas tomadas al azar,
lo cual puede darnos una idea equivocada. En este tipo de casos está bien
comprobar nuestras hipótesis realizando varios muestreos, teniendo en cuenta
que nunca se va a poder realizar un barrido completo al conjunto de datos,
por lo que no se pueden sacar conclusiones sólo con esto.

En las gráficas de la figura~\ref{fig:replays1} se pueden ver los distintos
recursos de una partida.

A primera vista, no parece que estas características vayan a ser demasiado
importantes para la predicción del resultado. Ni hay cambios bruscos,
ni diferencias significativas entre los valores de los recursos entre los
jugadores.


<<replays1, fig.cap = "Parámetros de tres partidas cualesquiera (1/5)">>=
replays.melt.plots[[1]]
@

\newpage

En las gráficas de la figura~\ref{fig:replays2} se aprecian los recursos
acumulados de cada jugador. Al igual que antes, no parece que vayan a tener
gran peso.

<<replays2, fig.cap = "Parámetros de tres partidas cualesquiera (2/5)">>=
replays.melt.plots[[2]]
@

\newpage

En la figura~\ref{fig:replays3} se empiezan a ver variables que parecen
importantes. Se muestran los valores de unidades de tierra, aire y
edificaciones. El jugador que pierde tiene una clara tendencia a perder unidades
y edificios de manera brusca, por lo que podría ser un buen indicativo del
resultado.

<<replays3, fig.cap = "Parámetros de tres partidas cualesquiera (3/5)">>=
replays.melt.plots[[3]]
@

\newpage

En esta figura~\ref{fig:replays4} se pueden observar los valores observados
por un jugador de otro de los valores de unidades y edificios. Son valores
muy volátiles por la mecánica del juego: en cuanto descubres una
región del mapa, descubres las unidades de los contrarios. A simple vista
no ofrecen ayuda, pero es muy probable que a un modelo de aprendizaje le vengan
muy bien estos atributos.

<<replays4, fig.cap = "Parámetros de tres partidas cualesquiera (4/5)">>=
replays.melt.plots[[4]]
@

\newpage

Por último, en~\ref{fig:replays5} se ven los recursos restantes del mapa
estimados. Parece que hay un jugador que sabe que hay cada vez menos recursos
en el mapa: este será quien más haya explorado. Es posible que por las
implicaciones que tiene, sea un atributo a considerar.

<<replays5, fig.cap = "Parámetros de tres partidas cualesquiera (5/5)">>=
replays.melt.plots[[5]]
@

Por otra parte, el conjunto de datos es tan completo en su estructura que no
se van a extraer ningún tipo de características derivadas. Algunas útiles
podrían ser el acumulado de recursos durante la partida los cuales ya están
reflejados en los datos. Otra característica interesante podría ser la
proporción de la partida que se ha jugado, pero supondría un problema si
quisiéramos cumplir el objetivo de implantarlo en un agente en tiempo real,
ya que no se sabría la duración de la partida hasta que no acabara.

\section{Modelado}
\label{sec:modelado}

A la vista de la gran cantidad de datos que han sido extraídos, hay que
tomar la decisión de con qué software se van a manejar. Como se comentó
en~\ref{cap:metodologia}, se ha utilizado la simbiosis de \emph{Scala},
\emph{Spark} y \emph{MLlib}, que es el estado del arte en entornos de
\emph{Big Data}. Proveen herramientas para que realizar análisis
en \emph{Big Data} en entornos distribuidos sea similar a hacerlo en un
entorno centralizado.

En un primer intento de sacar jugo a los datos tomados, se probó a utilizar
la clásica combinación de \emph{R} y el archiconocido paquete \emph{caret},
pero las implementaciones de algoritmos clásicos como \emph{Random Forest}
no eran capaces de manejar esta cantidad de datos, llenando la memoria
de la máquina en cuestión de minutos. Este hecho ha sido un gran incentivo
para tomar la decisión arriba propuesta: utilizar \emph{Spark} para el
tratamiento de datos. Aunque cabe mencionar que el problema a estudiar \textbf{NO}
es \emph{Big Data}, es bastante grande para tratarlo en una computadora personal.
Por tanto, el usar herramientas destinadas al tratamiento de grandes cantidades
de datos evitará tener problemas como los indicados.

Antes de comentar el modelado, organización de módulos y demás, se va a
realizar una breve introducción a \emph{Spark}, cómo funciona y cómo se utiliza.
Nótese que la versión utilizada de \emph{Spark} ha sido la 2.1.1, versión
lanzada el 2 de Mayo de 2017. Además, habría que destacar que la versión de la API
de la \emph{MLlib} utilizada es la más nueva, que trabaja con el tipo de dato
\emph{DataFrame}, al contrario que la antigua, que trabajaba con el tipo de dato
\emph{RDD}, (\texttt{Resilient Distributed Data}). Aunque coexisten ambas,
se recomienda utilizar la nueva, ya que es el camino que seguirá tomando
\emph{Spark} y \emph{MLlib} en el futuro próximo.

Primero se estudiarán varios conceptos esenciales para la comprensión del
ecosistema de trabajo que estamos utilizando, que están muy bien explicados
en \fnurl{la web de la API de \emph{Spark}}{https://spark.apache.org/docs/latest/ml-pipeline.html}.
\emph{Spark} y \emph{MLlib}
se basan en el concepto de \emph{Pipeline}, \emph{Tubería} en español.
\emph{MLlib} estandariza la API, pudiendo encadenar todo tipo de métodos para
aprendizaje automático de manera sencilla, como si de un flujo de trabajo
se tratara. Existen varios tipos de ``objetos'' que se pueden encadenar
dentro de un \emph{Pipeline}.

\begin{itemize}
  \item \emph{DataFrame}: Tipo de dato que almacena un conjunto de datos
  para aplicar aprendizaje sobre él. Este tipo de dato viene directamente del
  módulo \emph{SQL} para \emph{Spark}, lo que significa que podemos realizar
  cualquier operación de \emph{SQL} sobre éste directamente desde \emph{Spark},
  facilitando mucho la tarea de realizar operaciones sencillas pero potentes
  en \emph{SQL}, como agregaciones, agrupaciones y selecciones.
  Además, a partir de la versión 2.0 de \emph{Spark}, se pueden cargar directamente
  ficheros comunes de distinto tipo (como \texttt{csv} o \texttt{json}) sin
  necesidad de bibliotecas externas y sin complicaciones.
  \item \emph{Transformer}: Algoritmo que transforma un \emph{DataFrame} en
  otro \emph{DataFrame}. El más utilizado es un modelo de aprendizaje,
  que transforma un \emph{DataFrame} con datos en otro con datos y predicciones.
  Otro \emph{Transformer} muy usual es transformar alguna variable o un
  conjunto de ellas, para discretizarlas, normalizarlas, etc.
  \item \emph{Estimator}: Algoritmo que ajusta un \emph{DataFrame} para producir
  un \emph{Transformer}. Un algoritmo de aprendizaje es un \emph{Estimator}
  que proporciona un modelo, un \emph{Transformer}.
  \item \emph{Pipeline}: Estructura para encadenar \emph{Transformers} y
  \emph{Estimators} para producir un flujo de trabajo para aprendizaje.
  \item \emph{Parameter}: Parámetros para \emph{Transformers} y \emph{Estimators}.
\end{itemize}

Más específicamente, un \emph{Pipeline} es una secuencia de pasos, donde cada
uno es o bien un \emph{Transformer} o un \emph{Estimator}. Estos pasos tienen
un orden, determinado en tiempo de compilación, donde la entrada es un
\emph{DataFrame} que se va transformando con los operadores que constituyan el
\emph{Pipeline}. En la figura~\ref{fig:pipeline} se puede ver un ejemplo de
\emph{Pipeline} para un problema de clasificación de texto, donde los elementos
en azul son \emph{Transformers} y el elemento rojo, el algoritmo de modelado,
es un \emph{Estimator}. En la fila de abajo, se contemplan las modificaciones
que van sufriendo los datos hasta llegar al modelo final.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figure/ml-Pipeline.png}
  \caption{Ejemplo de \emph{Pipeline} para clasificación de texto.}
  \label{fig:pipeline}
\end{figure}

Además, estos \emph{Pipelines} pueden ser serializados y guardados en disco
para su posterior carga y utilización, lo cual facilita la labor de análisis
cuando hay que repetir el mismo proceso o cuando el tiempo de cómputo es muy
alto.

Antes de pasar al modelado en sí, hay que destacar un último detalle.
\emph{Spark} y la \emph{MLlib} precisan de que el conjunto de datos de modelado
esté de una forma determinada, con dos componentes:

\begin{itemize}
  \item \emph{Features}: Vector con componentes numéricas que contiene los
  atributos predictores del conjunto de datos.
  \item \emph{Label}: Etiqueta numérica del dato.
\end{itemize}

En cualquier caso, se pueden modificar los nombres de las columnas, pero no
su forma. Por tanto, si se carga un dataset en formato \texttt{csv}, por
ejemplo, habrá que tomar todas las variables predictoras y meterlas en un
vector de numéricos, y asegurarse de que la etiqueta también esté en dicho formato.

Una vez se tiene idea de cómo funcionan \emph{Spark} junto a la \emph{MLlib},
se pasa a describir cómo se ha organizado el modelado. Se han escrito dos
módulos en \emph{Scala}. Forman un paquete llamado \emph{StarCraft-Analytics}

El primero de ellos, llamado \emph{Modelling}, el cual almacena qué modelos
se van a ajustar y sus parámetros, realiza las siguientes funciones:

\begin{itemize}
  \item Carga los datos del dataset.
  \item Separa el dataset en entrenamiento y test, con proporciones 70\% y 30\%
  respectivamente, guardando las particiones en disco. Si existen, simplemente
  las carga, para mantener las particiones entre ejecuciones.
  \item Forma un \emph{Pipeline} cuya salida es el modelo aprendido, con los
  pasos necesarios para darle el formato necesario al \emph{DataFrame} para
  tratarlo con \emph{MLlib}.
  \item Con este \emph{Pipeline}, tunea los hiperparámetros del modelo
  utilizando validación cruzada con 10 particiones.
  \item Guarda en disco el modelo resultante para su posterior análisis
\end{itemize}

El \emph{Pipeline} es generado de forma dinámica según los modelos
que ya existan en disco, para no reaprender el mismo modelo en varias
ejecuciones distintas. Para ello se hace uso de la orientación a objetos
de \emph{Scala}. El \emph{Pipeline} generado tiene esta estructura.

\begin{itemize}
  \item \texttt{racesIndexer}: \emph{Transformer} del tipo \texttt{StringIndexer}
  que indexa una variable de tipo \texttt{String}. La etiqueta más frecuente
  se indexará con el valor \texttt{0.0}, la siguiente con \texttt{1.0}, y así
  sucesivamente. Se utiliza para indexar la variable que modela las razas
  implicadas en la partida, que es de tipo \texttt{String}.
  \item \texttt{winnerIndexer}: Similar a \texttt{racesIndexer}, pero se usa
  para la variable de salida, que también es de tipo \texttt{String}.
  \item \texttt{assembler}: \emph{Transformer} de tipo \texttt{VectorAssembler}
  que toma varias columnas de un \emph{DataFrame} y genera una nueva con
  todas las de entrada organizadas como un único vector. Este vector será el
  vector de variables predictoras que hablamos antes.
  \item \texttt{featureIndexer}: \emph{Transformer} del tipo \texttt{VectorIndexer}
  que se usa para detectar variables categóricas. Como la única variable
  categórica es la que modela las razas implicadas y tiene 6 posibles valores,
  se configura este paso del \emph{Pipeline} para que cualquier variable que
  tenga más de 6 valores, la tome como continua.
  \item El algoritmo de modelado.
\end{itemize}

Utilizando este \emph{Pipeline} se forma un objeto del tipo \emph{CrossValidator},
que optimiza un objeto del tipo \emph{Estimator} (cabe recordar que un
\emph{Pipeline} lo es) con un determinado grid de parámetros del tipo
\emph{ParamGrid}, utilizando
validación cruzada con 10 particiones según la medida deseada. Esto se consigue
gracias a un objeto del tipo \emph{MulticlassClassificationEvaluator}, el cual
se ha configurado para utilizar la precisión de clasificación. Se ha decidido esta
medida en detrimento del \emph{Area Under Curve}, (\emph{AUC}) porque hay
algunos modelos en \emph{MLlib}
que no tienen implementada la salida necesaria para calcularlo.

Así se consigue un modelo con unos parámetros más cerca de los óptimos para
cada algoritmo y para este problema. Cada uno de los modelos resultantes se
guardará en disco para su uso en el siguiente módulo.

Éste, llamado \emph{Testing}, realiza los siguientes pasos:

\begin{itemize}
  \item Carga las particiones de entrenamiento y test previamente almacenadas.
  \item Carga los modelos guardados en disco.
  \item Para cada modelo, calcula 3 medidas sobre el conjunto de test:
  precisión, \emph{AUC} y \emph{F1}. Guarda los resultados en disco.
  \item Extrae, de los modelos en los que sea posible, la importancia de las
  variables predictoras. No sólo es importante predecir, sino saber por qué
  se da dicha predicción. Guarda los resultados en disco.
\end{itemize}

Sólo queda decidir qué modelos se van a aprender de los datos. Por su
disponibilidad en \emph{Spark} y \emph{MLlib}, se han decidido tomar los
modelos y parámetros reflejados en la tabla~\ref{tab:modelos}. Se debe destacar
el caso de \emph{KNN}, que no está oficialmente en la \emph{MLlib}, por lo que
hay que cargarlo al ejecutar la aplicación. Esto se realiza automáticamente
con la interfaz de línea de comandos de \emph{Spark} sin problema, haciendo
una petición a la base de datos de paquetes de \emph{Spark},
\fnurl{spark-packages}{https://spark-packages.org/}.

Es por ese motivo por el que hay un pequeño problema con \emph{KNN}: dicho
paquete no tiene, por el momento, implementación de la interfaz
\emph{MLWritable}, que permite guardar un modelo en disco. Aún así los módulos
están preparados para que cuando dicha interfaz esté implementada, se puedan
usar con este modelo sin problema. Por el momento, la experimentación con este
modelo se ha realizado con la shell interactiva de \emph{Spark} sin mayor
inconveniente.

\begin{table}[]
\centering
\caption{Modelos junto a sus parámetros}
\label{tab:modelos}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccc@{}}
\toprule
Algoritmo & Parámetros fijos & Parámetros a tunear \\ \midrule
Regresión Logística & maxIter = 150 & regParam = (0.3, 0.5) \\
Naive Bayes &  & Smoothing = (1, 2) \\
Random Forest & numTrees = 150 & maxDepth = (5, 10) \\
Gradient Boosting Tree & numTrees = 150 & maxDepth = (5, 10) \\
Multilayer Perceptron & maxIter = 150 & Hidden Layers = (10,10) , (5, 4) \\
K Nearest Neighbours &  & K = (3, 5) \\ \bottomrule
\end{tabular}%
}
\end{table}
